# Attributing and Detecting Fake Images Generated by Known GANs

This repository contains the code for the paper **Attributing and Detecting Fake Images Generated by Known GANs** by *Matthew Joslin* and [*Shuang Hao*](https://www.utdallas.edu/~shao/) and published at the [2020 IEEE Deep Learning and Security Workshop](https://www.ieee-security.org/TC/SPW2020/DLS/).

> Abstract:
>
> The quality of GAN-generated fake images has
> improved significantly, and recent GAN approaches, such as
> StyleGAN, achieve near indistinguishability from real images for
> the naked eye. As a result, adversaries are attracted to using
> GAN-generated fake images for disinformation campaigns and
> fraud on social networks. However, training an image generation
> network to produce realistic-looking samples remains a time-consuming and difficult problem, so adversaries are more likely
> to use published GAN models to generate fake images.
> In this paper, we analyze the frequency domain to attribute and
> detect fake images generated by a known GAN model. We derive
> a similarity metric on the frequency domain and develop a new
> approach for GAN image attribution. We conduct experiments
> on four trained GAN models and two real image datasets. Our
> results show high attribution accuracy against real images and
> those from other GAN models. We further analyze our method
> under evasion attempts and find the frequency-based approach
> is comparatively robust.

## Citation

If you find this work useful for your research, please cite our paper:
```
@inproceedings{dls2020-gan-attribution,
    title = {{Attributing and Detecting Fake Images Generated by Known GANs}},
    author = {Joslin, Matthew and Hao, Shuang},
    booktitle = {Proceedings of the 3rd Deep Learning and Security Workshop, co-located with the 41st IEEE Symposium on Security and Privacy},
    series = {DLS},
    month = {May},
    year = {2020}
}
```

## Prerequisites

We recommend creating a virtual environment to isolate the dependencies using `virtualenv -p python3.7 gan-attr`. We have only tested with Python 3.7+.
Then install the required Python packages in the virtual environment using `pip install -r requirements.txt`.

## Datasets

For the real images, we use two popular face datasets:

* [CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html)
* [FFHQ](https://github.com/NVlabs/ffhq-dataset)

For the GAN images, we use three different architectures for a total of four models:

* [StyleGAN CelebA and FFHQ](https://github.com/NVlabs/stylegan/)
* [ProGAN CelebA](https://github.com/tkarras/progressive_growing_of_gans/)
* [StarGAN CelebA](https://github.com/yunjey/StarGAN/)

### Configuration

To modify the set of datasets to be processed, edit the `datasets.json` file. This file contains a list of dictionaries. Each dictionary describes one dataset. For example:

```json
{
    "dataset_name": "progan_celeba",
    "directory": "datasets/progan_celeba",
    "resolution": 1024,
    "type": "fake",
    "header": "ProGAN CelebA",
    "training_dataset": "progan_celeba"
}
```

The `dataset_name` will be used when naming output files, `directory` describes where the dataset is stored, the `resolution` describes how large the fingerprint should be, the `type` describes whether it contains `real` or `fake` images, the `header` is the pretty description of the dataset for graphs, etc., and the `training_dataset` identifies which set of real images was used to train the model.

## Execution

All of the processing steps are run from the `main.py` script. To use this script:

```
usage: main.py [-h] {split,fingerprint,testing} ...

Analyze a GAN for patterns.

positional arguments:
  {split,fingerprint,testing}
    split               Generate splits.
    fingerprint         Generate the fingerprints.
    testing             Perform the actual testing.

optional arguments:
  -h, --help            show this help message and exit
```

A basic use case should look like:
```
python3 main.py split
python3 main.py fingerprint
python3 main.py testing
```

### 1) Split Images

To split the images into fingerprint and testing sets. This step will generate one json file per dataset in the `split` directory. Recommended values for the split are 1,000 for the fingerprint and the rest for testing. By default, only fake datasets will be split.

```
usage: main.py split [-h] [--fingerprint FINGERPRINT] [--testing TESTING]
                     [--output_root OUTPUT_ROOT] [--split SPLIT]
                     [--no-allow-upsample] [--overwrite]

Generate the fingerprint and testing splits for a set of datasets.

optional arguments:
  -h, --help                    show this help message and exit
  --fingerprint FINGERPRINT     Number of images to use for the fingerprint (default: 1000)
  --testing TESTING             Number of images to use in testing (default: 1000)
  --output_root OUTPUT_ROOT     Root output directory (default: results)
  --split SPLIT                 Sub path to the split filenames directory (default: split)
  --no-allow-upsample           Only use images which are larger than the resolution in datasets.json (default: False)
  --overwrite                   Overwrite previous results. (default: False)
```

### 2) Generate Fingerprint

To generate the fingerprint for each of the datasets with a fingerprint split:

```
usage: main.py fingerprint [-h] [--output_root OUTPUT_ROOT] [--split SPLIT]
                           [--fingerprint FINGERPRINT] [--overwrite]

Generate the fingerprints for the datasets in datasets.json.

optional arguments:
  -h, --help                    show this help message and exit
  --output_root OUTPUT_ROOT     Root output directory (default: results)
  --split SPLIT                 Sub path to the split filenames directory (default: split)
  --fingerprint FINGERPRINT     Sub path to the fingerprint directory (default: fingerprint)
  --overwrite                   Overwrite previous results. (default: False)
```

### 3) Perform Testing

Finally, we measure the correlation of the test images with the fingerprints.
We output the accuracy as the total number of correct classifications divided by the total number of classifications.
We also output a json file containing a list of all the classification results where each entry is a dictionary.
Each entry contains the `dataset`, `filename`, and `fingerprint` which were used to perform the testing.
Additionally, we output the `correlation` and `threshold` used to perform the classification and we attach whether or not the image is `predicted_from_model` (the classification result) and whether or not the image `is_from_model` (the groundtruth).
A matching classification (`predicted_from_model`) and groundtruth (`is_from_model`) is counted as correct classification when calculating accuracy.

```json
{
    "dataset": "real_celeba",
    "filename": "../datasets/real_celeba/015457.jpg",
    "fingerprint": "progan_celeba",
    "correlation": -0.001657275466954418,
    "threshold": 0.011593234338272626,
    "predicted_from_model": false,
    "is_from_model": false
}
```

When performing the correlation we have several options:

* Threshold: when performing the testing we support providing a threshold for classification decisions. By default we will select the threshold which provides the optimal accuracy.
* Test cases: `all_to_all` compares all combinations of fingerprint to datasets, while `one_to_one` compares a fingerprint to the training set used to generate it. `both` includes the `all_to_all` and `one_to_one` cases.
* Evasion type: to simulate popular evasion techniques we provide three types `noise`, `blur`, and `jpeg`. Noise applies Gaussian noise, blur applies a box filter, and JPEG compresses the image. The default is no evasion or evasion level 0.
* Evasion level: each of the different evasion types have different meanings for the evasion level. For `noise` the evasion level refers to the average % change in the pixel value and is a float ranging between 0 and 1. For `blur` the evasion level describes how large the box filter should be and is an integer larger than 1. For `jpeg` the evasion level corresponds to the quality of the output image and is an integer ranging between 1 and 100.

```
usage: main.py testing [-h] [--output_root OUTPUT_ROOT] [--split SPLIT]
                       [--fingerprint FINGERPRINT] [--predictions PREDICTIONS]
                       [--correlations CORRELATIONS] [--temp TEMP]
                       [--test_cases {both,all_to_all,one_to_one}]
                       [--evasion_type {none,noise,blur,jpeg}]
                       [--evasion_level EVASION_LEVEL] [--threshold THRESHOLD]
                       [--overwrite]

Perform the testing for the datasets in datasets.json.

optional arguments:
  -h, --help            show this help message and exit
  --output_root OUTPUT_ROOT
                        Root output directory (default: results)
  --split SPLIT         Sub path to the split filenames directory (default: split)
  --fingerprint FINGERPRINT
                        Sub path to the fingerprint directory (default: fingerprint)
  --predictions PREDICTIONS
                        Sub path to the predictions directory (default: predictions)
  --correlations CORRELATIONS
                        Sub path to the correlations results directory (default: correlations)
  --temp TEMP           Sub path to the temp evasion image directory (default: temp)
  --test_cases {both,all_to_all,one_to_one}
                        Which test cases to try. All-to-all compares all
                        datasets with all fingerprints. One-to-one compares
                        each fingerprint with the true dataset and the
                        training dataset. Both covers all-to-all and one-to-one. (default: both)
  --evasion_type {none,noise,blur,jpeg}
                        What type of evasion to perform. See the README for
                        more information. (default: none)
  --evasion_level EVASION_LEVEL
                        The evasion level. Meaning depends on --evasion_type.
                        See README for more information. (default: 0)
  --threshold THRESHOLD
                        The threshold to use for prediction. Otherwise the
                        threshold is chosen for optimum accuracy for each test case. (default: None)
  --overwrite           Overwrite previous results. (default: False)
```

### 4) Clean-up

To restart with a fresh output, simply delete the results root directory (`--output_root`) which is set to `results` by default. This guarantees that all results are fresh. Additionally, specifying `--overwrite` for each step will also ensure fresh results.
